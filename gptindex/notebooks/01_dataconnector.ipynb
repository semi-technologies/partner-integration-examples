{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use GPT Index to read data stored in Weaviate.\n",
    "\n",
    "The weaviate instance in this notebook has been loaded with data from https://github.com/weaviate/weaviate-podcast-search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from weaviate import Client\n",
    "\n",
    "from gpt_index import (\n",
    "    GPTListIndex,\n",
    "    GPTTreeIndex,\n",
    "    Document,\n",
    ")\n",
    "\n",
    "from gpt_index.composability import ComposableGraph\n",
    "\n",
    "from gpt_index.readers.weaviate.reader import WeaviateReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make gpt index verbose\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_tree(tree_idx):\n",
    "    nodes = tree_idx.index_struct.root_nodes.values()\n",
    "    text = '\\n'.join(n.text for n in nodes)\n",
    "    doc = Document(text)\n",
    "    summary = GPTListIndex([doc]).query(\"summarize this conversation\").response\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the dataset in weaviate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEAVIATE_URL = \"http://weaviate:8080\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'gitHash': '5ce21bb',\n",
       "  'name': 'node1',\n",
       "  'shards': [{'class': 'PodClip', 'name': '2zbGNa7foWGF', 'objectCount': 394}],\n",
       "  'stats': {'objectCount': 394, 'shardCount': 1},\n",
       "  'status': 'HEALTHY',\n",
       "  'version': '1.17.3'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(WEAVIATE_URL)\n",
    "client.cluster.get_nodes_status()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the schema of PodClip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dataType': ['text'],\n",
       "  'description': 'The text content of the podcast clip',\n",
       "  'moduleConfig': {'text2vec-transformers': {'skip': False,\n",
       "    'vectorizeClassName': False,\n",
       "    'vectorizePropertyName': False}},\n",
       "  'name': 'content',\n",
       "  'tokenization': 'word'},\n",
       " {'dataType': ['string'],\n",
       "  'description': 'The speaker in the podcast',\n",
       "  'moduleConfig': {'text2vec-transformers': {'skip': True,\n",
       "    'vectorizeClassName': False,\n",
       "    'vectorizePropertyName': False}},\n",
       "  'name': 'speaker',\n",
       "  'tokenization': 'word'},\n",
       " {'dataType': ['int'],\n",
       "  'description': 'The podcast number.',\n",
       "  'moduleConfig': {'text2vec-transformers': {'skip': True,\n",
       "    'vectorizeClassName': False,\n",
       "    'vectorizePropertyName': False}},\n",
       "  'name': 'podNum'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.schema.get(\"PodClip\")[\"properties\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data in weaviate into gpt index documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_nums = [25, 26, 27, 28, 30, 31, 32, 33, 34]\n",
    "docs = []\n",
    "\n",
    "weaviate_reader = WeaviateReader(WEAVIATE_URL)\n",
    "\n",
    "\n",
    "for pod_num in pod_nums:\n",
    "    graphql_query = client.query\\\n",
    "        .get(class_name=\"PodClip\", properties=[\"content\", \"speaker\"])\\\n",
    "        .with_where({\n",
    "            \"path\": [\"podNum\"],\n",
    "            \"operator\": \"Equal\",\n",
    "            \"valueInt\": pod_num\n",
    "        })\\\n",
    "        .with_limit(1000)\\\n",
    "        .build()\n",
    "\n",
    "    doc = weaviate_reader.load_data(\n",
    "        graphql_query=graphql_query, separate_documents=False)[0]\n",
    "    doc.doc_id = pod_num\n",
    "\n",
    "    docs.append(doc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index each document into a tree index ðŸ’¸ðŸ’¸ðŸ’¸:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 3 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Building index from nodes: 3 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 12292 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [build_index_from_documents] Total LLM token usage: 12292 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 4 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Building index from nodes: 4 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 15552 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [build_index_from_documents] Total LLM token usage: 15552 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "tree_idxs = [GPTTreeIndex([doc]) for doc in docs[:2]]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the text of each tree so that we can compose it with another index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [build_index_from_documents] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 636 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [query] Total LLM token usage: 636 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [build_index_from_documents] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 877 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [query] Total LLM token usage: 877 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [query] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "for tree_idx in tree_idxs:\n",
    "    summary = summarize_tree(tree_idx)\n",
    "    tree_idx.set_text(summary)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put a list index on top of the tree index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [build_index_from_documents] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "list_idx = GPTListIndex(tree_idxs)\n",
    "data_in_gpt_index = ComposableGraph.build_from_index(list_idx)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Starting query: What is Mosaic ML?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Starting query: What is Mosaic ML?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 620 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [query] Total LLM token usage: 620 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Starting query: What is Mosaic ML?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Starting query: What is Mosaic ML?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:>[Level 0] Selected node: [1]/[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">[Level 0] Selected node: [1]/[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:>[Level 1] Selected node: [2]/[2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">[Level 1] Selected node: [2]/[2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4983 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [query] Total LLM token usage: 4983 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 5887 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [query] Total LLM token usage: 5887 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [query] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Mosaic ML?\"\n",
    "answer = data_in_gpt_index.query(query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ANSWER: Mosaic ML is an open source library that provides efficient methods for training large language models such as GPT-3. It also includes an orchestration stack as part of its Mosaic cloud, which allows users to train GPT-3 models for a starting price of $450,000. The goal of Mosaic ML is to drive the cost of training GPT-3 models down to as close to zero as possible.\n"
     ]
    }
   ],
   "source": [
    "print(answer.response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the source documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 1:\n",
      "5e890958-4af2-43b0-9770-c80788e1e95d\n",
      "\n",
      "Erik Bernhardsson and Etienne Dilocker discussed the power of vector models and the new databases and embedding models that have been developed in the past few years. They discussed the importance of making the trade-off between recall and latency explicit, and how to configure parameters to achieve high recall. They also discussed the use of mini-batching and matrix algebra for high performance model serving, the use of vector search providers and hybrid search, as well as two-stage pipeline approaches such as question-answer extraction. They concluded by discussing the importance of finding the right approach for the user's use case.\n",
      "\n",
      "Source 2:\n",
      "d40755b8-1c59-4308-877d-23295a43017a\n",
      "\n",
      "Connor Shorten and Jonathan Frankle discussed the latest update with Mosaic ML Cloud and training large language models, such as GPT-3. They discussed the importance of data volume, transfer learning, and pre-training BERT. They also discussed the applications of large language models, such as transfer learning for few shot learning and language models as databases. They discussed the compositional generalization of language models, such as GPT-2 and GPT-3, and how to query them. They also discussed the Mosaic Cloud platform, and the challenges of managing time in their respective roles at Mosaic and in science.\n",
      "\n",
      "Source 3:\n",
      "26\n",
      "efficiently. So I think the last time we chatted, we just released Composer, which is our open source library with all of our efficiency methods built in. And I think we had shortly thereafter released our ResNet recipe where we'd gotten like a 7x speed up in training ResNet50 on ImageNet over the standard NVIDIA baselines, which I'm kind of jealous. I'm not seeing a bunch of people doing lottery ticket experiments I humanly couldn't do because it was taking too long to train. And now people are able to do it overnight and I'm super jealous that people get to do that work now. But our latest release is for large language models for GPT-3 type models specifically. And it's been a lot of work on our part, but we've put together our own software stack for training these models. We've put together underneath that an orchestration stack, which is part of our Mosaic cloud that I'm sure we'll talk about a little bit. And the end result is we put out some prices. You want to train GPT-3 today, you can call me today. It's $450,000 to get to GPT-3 quality. We can talk more about where that number comes from and how it compares to other numbers. And I'll tell everyone that's the starting point. That's the baseline to beat. And first of all, if you beat that baseline, let me know. We'd love to obviously make it better. But second of all, our goal at Mosaic is going to be to drive that cost down as close to zero as possible over the next while. I set the goal informally to the team of getting to 100K sometime in the next\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sources = answer.source_nodes\n",
    "\n",
    "for i, source in enumerate(sources, 1):\n",
    "    print(f\"Source {i}:\")\n",
    "    print(source.doc_id)\n",
    "    print(source.source_text)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
